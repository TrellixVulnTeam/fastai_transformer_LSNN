{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.\n",
      "Warning:  apex was installed without --cuda_ext. Fused syncbn kernels will be unavailable.  Python fallbacks will be used instead.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedAdam will be unavailable.\n",
      "Warning:  apex was installed without --cuda_ext.  FusedLayerNorm will be unavailable.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from fastai.text import *\n",
    "from bert import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data/composers/notewise/piano_solo/note_range62/sample_freq12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs=16\n",
    "# bptt=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=2\n",
    "bptt=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextLMDataBunch.load(path, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = data.train_ds.vocab\n",
    "vocab_size = len(vocab.itos); vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos wait25 wait25 wait25 wait25 wait25 wait25 wa',\n",
       " array([  2, 124, 124, 124, ...,   9, 105,   9, 157]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = data.train_ds[0][0]\n",
    "t.text[:50], t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity - make sure outputs match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = language_model_learner(data, drop_mult=1, clip=.5, bptt=250)\n",
    "\n",
    "# ob = data.one_batch()\n",
    "\n",
    "# out = learn.model(ob[0].cuda())\n",
    "\n",
    "# out[0].shape\n",
    "\n",
    "# len(out[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Arch\n",
    "Paper: https://arxiv.org/abs/1706.03762  \n",
    "Inspiration: https://github.com/jadore801120/attention-is-all-you-need-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config, bert_model_embedding_weights):\n",
    "        super(BertLMPredictionHead, self).__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n",
    "                                 bert_model_embedding_weights.size(0),\n",
    "                                 bias=False)\n",
    "        self.decoder.weight = bert_model_embedding_weights\n",
    "        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states) + self.bias\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config, bert_model_embedding_weights):\n",
    "        super(BertOnlyMLMHead, self).__init__()\n",
    "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "    \n",
    "\n",
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config, bert_model_embedding_weights):\n",
    "        super(BertPreTrainingHeads, self).__init__()\n",
    "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score\n",
    "\n",
    "\n",
    "class PreTrainedBertModel(nn.Module):\n",
    "    \"\"\" An abstract class to handle weights initialization and\n",
    "        a simple interface for dowloading and loading pretrained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(PreTrainedBertModel, self).__init__()\n",
    "        if not isinstance(config, BertConfig):\n",
    "            raise ValueError(\n",
    "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
    "                \"To create a model from a Google pretrained model use \"\n",
    "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
    "                    self.__class__.__name__, self.__class__.__name__\n",
    "                ))\n",
    "        self.config = config\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "\n",
    "\n",
    "class BertModel(PreTrainedBertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask,\n",
    "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "class BertForPreTraining(PreTrainedBertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForPreTraining, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n",
    "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                                   output_all_encoded_layers=False)\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        if masked_lm_labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "            return total_loss\n",
    "        else:\n",
    "            return prediction_scores\n",
    "#             return prediction_scores, seq_relationship_score\n",
    "\n",
    "class BertForUnidirectionalMaskedLM(PreTrainedBertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForMaskedLM, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n",
    "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                       output_all_encoded_layers=False)\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        if masked_lm_labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
    "            return masked_lm_loss\n",
    "        else:\n",
    "            return prediction_scores\n",
    "\n",
    "\n",
    "class BertForMaskedLM(PreTrainedBertModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForMaskedLM, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n",
    "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                       output_all_encoded_layers=False)\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        if masked_lm_labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
    "            return masked_lm_loss\n",
    "        else:\n",
    "            return prediction_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = embedding size\n",
    "# attention_heads = multi-attention heads\n",
    "config = BertConfig(vocab_size, hidden_size=12, \n",
    "                    num_hidden_layers=4, intermediate_size=24, num_attention_heads=6)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForPreTraining(config).cuda()\n",
    "model.reset = lambda: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(158, 12)\n",
       "    (position_embeddings): Embedding(512, 12)\n",
       "    (token_type_embeddings): Embedding(2, 12)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (key): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (value): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=12, out_features=24, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=24, out_features=12, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (key): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (value): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=12, out_features=24, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=24, out_features=12, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (key): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (value): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=12, out_features=24, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=24, out_features=12, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (key): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (value): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=12, out_features=12, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=12, out_features=24, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=24, out_features=12, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob = data.one_batch(); ob[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.bert(ob[0].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertDecoder, self).__init__()\n",
    "        layer = BertLayer(config)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13, 12])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0].shape # batch_size, bptt, embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1].shape # batch_size, embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=10\n",
    "np_mask = np.triu(np.full([max_length, max_length], -np.inf), 1)\n",
    "torch_mask = torch.from_numpy(np_mask).type(torch.FloatTensor)\n",
    "\n",
    "# Reshape to 4D Tensor to handle multiple heads\n",
    "# return torch_mask.unsqueeze(0).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 3.2806e-02,  7.9190e-03,  1.8894e-02,  1.8988e-02,  3.0615e-03,\n",
       "           -2.5271e-03,  3.5465e-02,  4.3688e-02,  3.0934e-02,  3.0623e-02,\n",
       "           -2.8933e-02,  1.0982e-02],\n",
       "          [ 3.2090e-02,  1.3790e-02,  1.8600e-02,  1.9114e-02,  5.0668e-03,\n",
       "           -5.5195e-03,  3.4787e-02,  4.8168e-02,  3.0979e-02,  3.3365e-02,\n",
       "           -3.6103e-02,  1.1477e-02],\n",
       "          [ 2.9084e-02,  1.3604e-02,  1.8023e-02,  1.3680e-02,  1.3718e-02,\n",
       "           -9.9367e-04,  3.7833e-02,  3.9668e-02,  3.0174e-02,  4.0612e-02,\n",
       "           -2.6636e-02,  1.9637e-02],\n",
       "          [ 3.0140e-02,  1.5994e-02,  1.8480e-02,  1.7107e-02,  5.7300e-03,\n",
       "           -8.8697e-05,  3.5898e-02,  5.2620e-02,  3.0628e-02,  4.0814e-02,\n",
       "           -3.8358e-02,  1.8999e-02],\n",
       "          [ 3.4333e-02,  1.6681e-02,  1.9207e-02,  1.8105e-02,  4.0070e-03,\n",
       "           -1.4917e-03,  3.3758e-02,  5.3686e-02,  3.0982e-02,  3.2102e-02,\n",
       "           -3.7432e-02,  1.6759e-02],\n",
       "          [ 3.5298e-02,  1.5952e-02,  1.9269e-02,  1.7380e-02, -1.4897e-03,\n",
       "            4.3347e-05,  3.4042e-02,  5.3831e-02,  3.0557e-02,  5.2966e-02,\n",
       "           -1.5483e-02,  2.2385e-02],\n",
       "          [ 2.7339e-02,  2.1026e-02,  1.9368e-02,  1.6807e-02,  7.7057e-03,\n",
       "           -1.4627e-03,  3.5541e-02,  6.3013e-02,  2.9625e-02,  5.3029e-02,\n",
       "           -2.7895e-02,  2.0203e-02],\n",
       "          [ 2.8065e-02,  1.3082e-02,  1.8744e-02,  9.4527e-03,  7.0657e-03,\n",
       "            2.8222e-03,  3.6354e-02,  6.3958e-02,  2.9501e-02,  5.4439e-02,\n",
       "           -1.5292e-02,  2.0763e-02],\n",
       "          [ 3.0103e-02,  1.3328e-02,  1.8718e-02,  1.8135e-02,  5.0343e-03,\n",
       "           -2.3520e-03,  3.5671e-02,  5.9141e-02,  3.0153e-02,  4.0785e-02,\n",
       "           -3.4990e-02,  1.0185e-02],\n",
       "          [ 3.5676e-02,  1.3974e-02,  1.9247e-02,  1.7544e-02,  6.1987e-03,\n",
       "           -4.3522e-03,  3.6323e-02,  5.4916e-02,  3.0481e-02,  2.8687e-02,\n",
       "           -3.4408e-02,  1.0776e-02],\n",
       "          [ 3.3442e-02,  2.3866e-02,  2.0140e-02,  2.0496e-02,  7.2668e-03,\n",
       "           -4.7567e-03,  3.4643e-02,  4.8385e-02,  3.0330e-02,  4.1422e-02,\n",
       "           -3.3461e-02,  2.0440e-02],\n",
       "          [ 3.4480e-02,  9.6014e-03,  1.9490e-02,  1.9209e-02, -1.1287e-03,\n",
       "           -2.6408e-03,  3.4801e-02,  4.7301e-02,  3.1263e-02,  2.8277e-02,\n",
       "           -3.3632e-02,  1.3697e-02],\n",
       "          [ 3.2622e-02,  1.4770e-02,  1.9324e-02,  1.8242e-02, -7.4037e-04,\n",
       "           -6.8315e-04,  3.5519e-02,  7.5980e-02,  2.9671e-02,  4.4735e-02,\n",
       "           -2.2618e-02,  1.7288e-02]],\n",
       " \n",
       "         [[ 3.0503e-02,  2.0177e-03,  2.0467e-02,  1.4718e-02,  1.2587e-03,\n",
       "           -6.0362e-04,  3.7895e-02,  4.3668e-02,  3.0040e-02,  2.6654e-02,\n",
       "           -2.4896e-02,  1.2778e-02],\n",
       "          [ 3.2278e-02,  1.4760e-02,  1.8798e-02,  1.1884e-02,  1.1025e-02,\n",
       "            2.4909e-03,  3.9595e-02,  6.2507e-02,  2.9031e-02,  4.4478e-02,\n",
       "           -2.4125e-02,  1.9720e-02],\n",
       "          [ 2.6566e-02,  1.1956e-02,  1.8401e-02,  1.3422e-02,  1.7486e-03,\n",
       "           -1.5720e-03,  3.2767e-02,  3.4868e-02,  3.1844e-02,  4.5326e-02,\n",
       "           -3.4364e-02,  2.0764e-02],\n",
       "          [ 3.2075e-02,  1.6574e-02,  1.9467e-02,  2.0855e-02, -1.8705e-03,\n",
       "           -4.7907e-03,  3.5730e-02,  5.3736e-02,  3.1516e-02,  3.7147e-02,\n",
       "           -4.0977e-02,  1.7764e-02],\n",
       "          [ 3.4831e-02,  1.5970e-02,  1.9565e-02,  1.8986e-02,  1.5382e-03,\n",
       "           -4.0335e-03,  3.2642e-02,  4.9822e-02,  3.1608e-02,  2.8389e-02,\n",
       "           -3.7791e-02,  1.5107e-02],\n",
       "          [ 3.1042e-02,  1.6249e-02,  1.9711e-02,  1.4964e-02, -2.9962e-03,\n",
       "           -1.7689e-03,  3.5203e-02,  6.6021e-02,  3.0143e-02,  5.7216e-02,\n",
       "           -1.3822e-02,  2.3830e-02],\n",
       "          [ 3.8629e-02,  1.1077e-02,  1.9342e-02,  1.8592e-02,  3.0092e-03,\n",
       "           -1.4219e-03,  3.3122e-02,  3.8124e-02,  3.1327e-02,  3.0765e-02,\n",
       "           -2.5443e-02,  1.1722e-02],\n",
       "          [ 2.3947e-02,  9.1330e-03,  1.8687e-02,  1.5781e-02,  7.2672e-03,\n",
       "            5.6423e-05,  3.8365e-02,  4.5547e-02,  2.9398e-02,  5.7939e-02,\n",
       "           -2.7228e-02,  1.0311e-02],\n",
       "          [ 3.0212e-02,  3.4525e-03,  2.0522e-02,  1.7042e-02,  1.3156e-04,\n",
       "           -4.2448e-03,  3.9204e-02,  4.1111e-02,  2.9862e-02,  3.5407e-02,\n",
       "           -3.1852e-02,  5.9324e-03],\n",
       "          [ 3.6787e-02,  1.0577e-02,  1.9079e-02,  1.3722e-02,  4.8737e-03,\n",
       "            1.1760e-03,  3.7765e-02,  5.3612e-02,  3.0266e-02,  2.6455e-02,\n",
       "           -2.9369e-02,  2.0251e-02],\n",
       "          [ 3.1772e-02,  1.0306e-02,  1.9082e-02,  1.5523e-02,  6.7883e-03,\n",
       "           -1.7367e-03,  3.8654e-02,  4.9137e-02,  3.0402e-02,  3.1947e-02,\n",
       "           -3.9899e-02,  1.0769e-02],\n",
       "          [ 3.2529e-02,  1.2692e-02,  1.9575e-02,  1.9129e-02, -2.9423e-03,\n",
       "           -4.1761e-03,  3.5148e-02,  5.5166e-02,  3.1420e-02,  3.0933e-02,\n",
       "           -3.8655e-02,  1.6395e-02],\n",
       "          [ 3.5795e-02,  1.1579e-02,  1.9293e-02,  1.9090e-02, -2.1428e-03,\n",
       "           -6.7439e-03,  3.3897e-02,  5.2637e-02,  3.1243e-02,  3.4657e-02,\n",
       "           -2.2511e-02,  1.2783e-02]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.0543,  0.0144, -0.0236,  0.0364,  0.0303,  0.0271, -0.0067,\n",
       "            0.0478, -0.0162, -0.0270,  0.0446, -0.0649],\n",
       "          [ 0.0537,  0.0175, -0.0225,  0.0362,  0.0308,  0.0270, -0.0063,\n",
       "            0.0476, -0.0160, -0.0270,  0.0458, -0.0642],\n",
       "          [ 0.0540,  0.0170, -0.0196,  0.0349,  0.0317,  0.0269, -0.0072,\n",
       "            0.0475, -0.0161, -0.0272,  0.0480, -0.0586],\n",
       "          [ 0.0541,  0.0195, -0.0210,  0.0355,  0.0301,  0.0272, -0.0064,\n",
       "            0.0476, -0.0157, -0.0273,  0.0480, -0.0601],\n",
       "          [ 0.0548,  0.0192, -0.0223,  0.0358,  0.0301,  0.0272, -0.0061,\n",
       "            0.0478, -0.0158, -0.0272,  0.0470, -0.0608],\n",
       "          [ 0.0562,  0.0156, -0.0173,  0.0356,  0.0281,  0.0268, -0.0054,\n",
       "            0.0497, -0.0164, -0.0275,  0.0433, -0.0611],\n",
       "          [ 0.0530,  0.0195, -0.0183,  0.0353,  0.0301,  0.0269, -0.0057,\n",
       "            0.0491, -0.0158, -0.0275,  0.0461, -0.0637],\n",
       "          [ 0.0544,  0.0145, -0.0166,  0.0340,  0.0299,  0.0270, -0.0058,\n",
       "            0.0508, -0.0164, -0.0277,  0.0439, -0.0657],\n",
       "          [ 0.0533,  0.0172, -0.0212,  0.0359,  0.0305,  0.0272, -0.0061,\n",
       "            0.0484, -0.0159, -0.0272,  0.0454, -0.0686],\n",
       "          [ 0.0544,  0.0168, -0.0222,  0.0359,  0.0311,  0.0271, -0.0063,\n",
       "            0.0483, -0.0160, -0.0270,  0.0454, -0.0658],\n",
       "          [ 0.0538,  0.0208, -0.0210,  0.0360,  0.0305,  0.0268, -0.0059,\n",
       "            0.0476, -0.0157, -0.0271,  0.0473, -0.0581],\n",
       "          [ 0.0548,  0.0159, -0.0245,  0.0364,  0.0297,  0.0272, -0.0064,\n",
       "            0.0478, -0.0161, -0.0271,  0.0452, -0.0614],\n",
       "          [ 0.0548,  0.0162, -0.0184,  0.0358,  0.0289,  0.0270, -0.0054,\n",
       "            0.0506, -0.0161, -0.0275,  0.0425, -0.0674]],\n",
       " \n",
       "         [[ 0.0540,  0.0108, -0.0271,  0.0359,  0.0302,  0.0273, -0.0073,\n",
       "            0.0484, -0.0165, -0.0270,  0.0439, -0.0620],\n",
       "          [ 0.0549,  0.0163, -0.0172,  0.0344,  0.0308,  0.0271, -0.0065,\n",
       "            0.0496, -0.0159, -0.0275,  0.0464, -0.0642],\n",
       "          [ 0.0537,  0.0180, -0.0239,  0.0353,  0.0297,  0.0272, -0.0064,\n",
       "            0.0469, -0.0162, -0.0273,  0.0481, -0.0547],\n",
       "          [ 0.0539,  0.0192, -0.0229,  0.0363,  0.0294,  0.0271, -0.0062,\n",
       "            0.0477, -0.0158, -0.0272,  0.0466, -0.0592],\n",
       "          [ 0.0546,  0.0187, -0.0238,  0.0361,  0.0302,  0.0271, -0.0060,\n",
       "            0.0476, -0.0159, -0.0271,  0.0462, -0.0602],\n",
       "          [ 0.0549,  0.0153, -0.0173,  0.0353,  0.0280,  0.0267, -0.0051,\n",
       "            0.0510, -0.0165, -0.0276,  0.0419, -0.0618],\n",
       "          [ 0.0560,  0.0147, -0.0230,  0.0363,  0.0302,  0.0271, -0.0062,\n",
       "            0.0475, -0.0163, -0.0269,  0.0445, -0.0643],\n",
       "          [ 0.0520,  0.0146, -0.0208,  0.0357,  0.0308,  0.0272, -0.0066,\n",
       "            0.0483, -0.0161, -0.0272,  0.0451, -0.0712],\n",
       "          [ 0.0526,  0.0120, -0.0266,  0.0364,  0.0307,  0.0273, -0.0070,\n",
       "            0.0479, -0.0163, -0.0268,  0.0439, -0.0675],\n",
       "          [ 0.0565,  0.0158, -0.0218,  0.0351,  0.0301,  0.0273, -0.0068,\n",
       "            0.0486, -0.0161, -0.0273,  0.0466, -0.0576],\n",
       "          [ 0.0536,  0.0166, -0.0236,  0.0357,  0.0311,  0.0274, -0.0069,\n",
       "            0.0476, -0.0159, -0.0271,  0.0469, -0.0647],\n",
       "          [ 0.0542,  0.0177, -0.0240,  0.0362,  0.0294,  0.0272, -0.0062,\n",
       "            0.0481, -0.0160, -0.0272,  0.0458, -0.0596],\n",
       "          [ 0.0549,  0.0141, -0.0217,  0.0365,  0.0295,  0.0267, -0.0058,\n",
       "            0.0491, -0.0165, -0.0270,  0.0421, -0.0647]]], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 4.4335e-02,  9.4179e-03, -7.2741e-02, -1.4991e-02,  3.0634e-02,\n",
       "            1.5291e-02, -1.5248e-02,  8.2118e-03, -3.7297e-02,  7.0052e-05,\n",
       "           -2.7754e-02, -1.1311e-02],\n",
       "          [ 4.5041e-02,  9.0644e-03, -7.2232e-02, -1.4916e-02,  3.0675e-02,\n",
       "            1.5371e-02, -1.5349e-02,  7.9505e-03, -3.7442e-02, -5.9830e-05,\n",
       "           -2.8080e-02, -1.1305e-02],\n",
       "          [ 4.3266e-02,  9.0146e-03, -7.1034e-02, -1.4700e-02,  3.0822e-02,\n",
       "            1.5410e-02, -1.5479e-02,  7.8913e-03, -3.8328e-02, -3.5966e-04,\n",
       "           -2.9159e-02, -1.0790e-02],\n",
       "          [ 4.3714e-02,  8.7629e-03, -7.1439e-02, -1.4747e-02,  3.0641e-02,\n",
       "            1.5360e-02, -1.5509e-02,  7.7666e-03, -3.7996e-02, -3.1164e-04,\n",
       "           -2.8963e-02, -1.0927e-02],\n",
       "          [ 4.3446e-02,  8.8944e-03, -7.2147e-02, -1.4804e-02,  3.0581e-02,\n",
       "            1.5358e-02, -1.5410e-02,  7.8199e-03, -3.7879e-02, -2.7445e-04,\n",
       "           -2.8494e-02, -1.0975e-02],\n",
       "          [ 4.2639e-02,  9.7739e-03, -7.2099e-02, -1.4861e-02,  2.9967e-02,\n",
       "            1.5632e-02, -1.4810e-02,  8.6952e-03, -3.8087e-02, -4.5733e-04,\n",
       "           -2.7432e-02, -1.1019e-02],\n",
       "          [ 4.6363e-02,  8.9427e-03, -7.1216e-02, -1.4781e-02,  3.0466e-02,\n",
       "            1.5543e-02, -1.5237e-02,  8.3362e-03, -3.7715e-02, -3.1524e-04,\n",
       "           -2.8207e-02, -1.1286e-02],\n",
       "          [ 4.5593e-02,  9.7905e-03, -7.1434e-02, -1.4801e-02,  3.0273e-02,\n",
       "            1.5533e-02, -1.4885e-02,  9.1939e-03, -3.7544e-02, -2.6177e-04,\n",
       "           -2.7604e-02, -1.1425e-02],\n",
       "          [ 4.6663e-02,  9.1790e-03, -7.1936e-02, -1.4956e-02,  3.0610e-02,\n",
       "            1.5364e-02, -1.5254e-02,  8.2378e-03, -3.7070e-02,  1.2693e-04,\n",
       "           -2.7862e-02, -1.1709e-02],\n",
       "          [ 4.4954e-02,  9.2321e-03, -7.2360e-02, -1.4930e-02,  3.0643e-02,\n",
       "            1.5404e-02, -1.5247e-02,  8.2236e-03, -3.7261e-02,  6.3302e-06,\n",
       "           -2.7875e-02, -1.1448e-02],\n",
       "          [ 4.3954e-02,  8.7129e-03, -7.1753e-02, -1.4755e-02,  3.0630e-02,\n",
       "            1.5508e-02, -1.5460e-02,  7.7255e-03, -3.8275e-02, -4.4394e-04,\n",
       "           -2.8601e-02, -1.0779e-02],\n",
       "          [ 4.3008e-02,  9.2290e-03, -7.3010e-02, -1.4938e-02,  3.0573e-02,\n",
       "            1.5294e-02, -1.5327e-02,  8.1695e-03, -3.7647e-02, -3.8943e-05,\n",
       "           -2.7959e-02, -1.1009e-02],\n",
       "          [ 4.6003e-02,  9.7258e-03, -7.2351e-02, -1.4955e-02,  3.0080e-02,\n",
       "            1.5579e-02, -1.4779e-02,  8.9857e-03, -3.7145e-02, -1.8013e-04,\n",
       "           -2.6810e-02, -1.1590e-02]],\n",
       " \n",
       "         [[ 4.3255e-02,  9.6312e-03, -7.3556e-02, -1.4986e-02,  3.0729e-02,\n",
       "            1.5100e-02, -1.5329e-02,  8.7847e-03, -3.7303e-02,  1.6027e-04,\n",
       "           -2.7669e-02, -1.0960e-02],\n",
       "          [ 4.4528e-02,  9.3845e-03, -7.1056e-02, -1.4766e-02,  3.0508e-02,\n",
       "            1.5501e-02, -1.5165e-02,  8.6058e-03, -3.7696e-02, -2.1092e-04,\n",
       "           -2.8457e-02, -1.1342e-02],\n",
       "          [ 4.2029e-02,  8.6648e-03, -7.2046e-02, -1.4693e-02,  3.0712e-02,\n",
       "            1.5238e-02, -1.5671e-02,  7.7395e-03, -3.8598e-02, -3.8926e-04,\n",
       "           -2.9182e-02, -1.0364e-02],\n",
       "          [ 4.3715e-02,  8.8401e-03, -7.2249e-02, -1.4823e-02,  3.0544e-02,\n",
       "            1.5360e-02, -1.5450e-02,  7.9268e-03, -3.7956e-02, -2.9784e-04,\n",
       "           -2.8411e-02, -1.0822e-02],\n",
       "          [ 4.3553e-02,  8.8490e-03, -7.2651e-02, -1.4849e-02,  3.0608e-02,\n",
       "            1.5358e-02, -1.5406e-02,  7.8825e-03, -3.7891e-02, -2.1491e-04,\n",
       "           -2.8239e-02, -1.0917e-02],\n",
       "          [ 4.4547e-02,  9.8977e-03, -7.2302e-02, -1.4861e-02,  2.9887e-02,\n",
       "            1.5661e-02, -1.4686e-02,  9.3434e-03, -3.7942e-02, -4.6476e-04,\n",
       "           -2.6830e-02, -1.1057e-02],\n",
       "          [ 4.2700e-02,  9.4997e-03, -7.2924e-02, -1.4988e-02,  3.0534e-02,\n",
       "            1.5380e-02, -1.5153e-02,  7.9840e-03, -3.7509e-02, -5.9446e-06,\n",
       "           -2.7718e-02, -1.1285e-02],\n",
       "          [ 4.8087e-02,  9.3562e-03, -7.1543e-02, -1.4986e-02,  3.0720e-02,\n",
       "            1.5247e-02, -1.5280e-02,  8.3797e-03, -3.6758e-02,  2.9893e-04,\n",
       "           -2.7887e-02, -1.1933e-02],\n",
       "          [ 4.5874e-02,  9.4962e-03, -7.3224e-02, -1.5070e-02,  3.0819e-02,\n",
       "            1.5115e-02, -1.5335e-02,  8.6503e-03, -3.6735e-02,  3.8791e-04,\n",
       "           -2.7468e-02, -1.1486e-02],\n",
       "          [ 4.0759e-02,  9.2892e-03, -7.2299e-02, -1.4758e-02,  3.0526e-02,\n",
       "            1.5411e-02, -1.5319e-02,  8.2150e-03, -3.8118e-02, -3.9814e-04,\n",
       "           -2.8563e-02, -1.0640e-02],\n",
       "          [ 4.4775e-02,  9.0097e-03, -7.2122e-02, -1.4886e-02,  3.0841e-02,\n",
       "            1.5233e-02, -1.5500e-02,  7.9915e-03, -3.7392e-02,  9.8228e-05,\n",
       "           -2.8517e-02, -1.1330e-02],\n",
       "          [ 4.3494e-02,  9.0191e-03, -7.2719e-02, -1.4860e-02,  3.0527e-02,\n",
       "            1.5312e-02, -1.5390e-02,  8.1677e-03, -3.7840e-02, -2.1493e-04,\n",
       "           -2.8131e-02, -1.0836e-02],\n",
       "          [ 4.4457e-02,  9.7922e-03, -7.3175e-02, -1.5039e-02,  3.0275e-02,\n",
       "            1.5502e-02, -1.4881e-02,  8.7513e-03, -3.7369e-02, -5.4870e-05,\n",
       "           -2.6778e-02, -1.1327e-02]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[[ 2.4434e-02, -1.5797e-02, -7.7356e-02, -3.8907e-03, -4.2669e-02,\n",
       "           -8.9518e-03, -5.2974e-06, -7.8461e-03,  7.9833e-03,  2.5620e-02,\n",
       "            2.5772e-03, -3.0055e-02],\n",
       "          [ 2.4443e-02, -1.5802e-02, -7.7107e-02, -3.8817e-03, -4.2583e-02,\n",
       "           -8.9840e-03,  1.4044e-04, -7.8523e-03,  7.9493e-03,  2.5565e-02,\n",
       "            2.6086e-03, -3.0048e-02],\n",
       "          [ 2.4545e-02, -1.5819e-02, -7.6281e-02, -3.8251e-03, -4.2387e-02,\n",
       "           -9.0914e-03,  2.2780e-04, -7.6896e-03,  7.8670e-03,  2.5395e-02,\n",
       "            2.6537e-03, -3.0278e-02],\n",
       "          [ 2.4517e-02, -1.5803e-02, -7.6609e-02, -3.8335e-03, -4.2460e-02,\n",
       "           -9.0652e-03,  2.1302e-04, -7.7451e-03,  7.9093e-03,  2.5373e-02,\n",
       "            2.6424e-03, -3.0188e-02],\n",
       "          [ 2.4501e-02, -1.5782e-02, -7.7002e-02, -3.7596e-03, -4.2453e-02,\n",
       "           -9.0414e-03,  1.6877e-04, -7.7972e-03,  7.9310e-03,  2.5415e-02,\n",
       "            2.6173e-03, -3.0213e-02],\n",
       "          [ 2.4470e-02, -1.5795e-02, -7.7093e-02, -3.6862e-03, -4.2729e-02,\n",
       "           -9.0544e-03, -4.2375e-06, -7.7052e-03,  7.9496e-03,  2.5705e-02,\n",
       "            2.5766e-03, -3.0203e-02],\n",
       "          [ 2.4412e-02, -1.5811e-02, -7.6711e-02, -4.0028e-03, -4.2661e-02,\n",
       "           -8.9987e-03,  1.5300e-04, -7.7351e-03,  7.8950e-03,  2.5585e-02,\n",
       "            2.6250e-03, -3.0032e-02],\n",
       "          [ 2.4406e-02, -1.5825e-02, -7.6929e-02, -3.9292e-03, -4.2822e-02,\n",
       "           -8.9573e-03,  7.2595e-05, -7.5252e-03,  7.9348e-03,  2.5702e-02,\n",
       "            2.6111e-03, -2.9944e-02],\n",
       "          [ 2.4389e-02, -1.5812e-02, -7.7132e-02, -4.0416e-03, -4.2700e-02,\n",
       "           -8.9323e-03,  1.2934e-04, -7.8367e-03,  7.9659e-03,  2.5670e-02,\n",
       "            2.6134e-03, -2.9844e-02],\n",
       "          [ 2.4424e-02, -1.5800e-02, -7.7178e-02, -3.9175e-03, -4.2634e-02,\n",
       "           -8.9766e-03,  1.0615e-04, -7.8519e-03,  7.9755e-03,  2.5631e-02,\n",
       "            2.6016e-03, -2.9992e-02],\n",
       "          [ 2.4479e-02, -1.5791e-02, -7.6761e-02, -3.8283e-03, -4.2490e-02,\n",
       "           -9.0695e-03,  1.6107e-04, -7.8595e-03,  7.9075e-03,  2.5473e-02,\n",
       "            2.6109e-03, -3.0282e-02],\n",
       "          [ 2.4492e-02, -1.5788e-02, -7.7255e-02, -3.7379e-03, -4.2512e-02,\n",
       "           -9.0026e-03,  1.2095e-04, -7.7895e-03,  7.9740e-03,  2.5492e-02,\n",
       "            2.5942e-03, -3.0207e-02],\n",
       "          [ 2.4341e-02, -1.5804e-02, -7.7351e-02, -4.0028e-03, -4.2868e-02,\n",
       "           -8.9186e-03,  2.3114e-05, -7.7321e-03,  7.9877e-03,  2.5774e-02,\n",
       "            2.5764e-03, -2.9900e-02]],\n",
       " \n",
       "         [[ 2.4487e-02, -1.5792e-02, -7.7561e-02, -3.7446e-03, -4.2589e-02,\n",
       "           -8.9154e-03,  1.2221e-04, -7.6364e-03,  8.0106e-03,  2.5488e-02,\n",
       "            2.5788e-03, -3.0184e-02],\n",
       "          [ 2.4461e-02, -1.5820e-02, -7.6565e-02, -3.9546e-03, -4.2596e-02,\n",
       "           -9.0248e-03,  1.2759e-04, -7.6322e-03,  7.9094e-03,  2.5588e-02,\n",
       "            2.6419e-03, -3.0008e-02],\n",
       "          [ 2.4610e-02, -1.5790e-02, -7.6657e-02, -3.6588e-03, -4.2320e-02,\n",
       "           -9.0806e-03,  2.0704e-04, -7.7178e-03,  7.8799e-03,  2.5305e-02,\n",
       "            2.6219e-03, -3.0491e-02],\n",
       "          [ 2.4494e-02, -1.5792e-02, -7.6883e-02, -3.7908e-03, -4.2472e-02,\n",
       "           -9.0408e-03,  1.7736e-04, -7.7643e-03,  7.9247e-03,  2.5410e-02,\n",
       "            2.6126e-03, -3.0284e-02],\n",
       "          [ 2.4497e-02, -1.5788e-02, -7.7044e-02, -3.7627e-03, -4.2470e-02,\n",
       "           -9.0253e-03,  1.6099e-04, -7.7952e-03,  7.9366e-03,  2.5426e-02,\n",
       "            2.6053e-03, -3.0268e-02],\n",
       "          [ 2.4384e-02, -1.5799e-02, -7.7154e-02, -3.8594e-03, -4.2829e-02,\n",
       "           -8.9824e-03, -8.7913e-05, -7.5658e-03,  7.9487e-03,  2.5697e-02,\n",
       "            2.5682e-03, -3.0128e-02],\n",
       "          [ 2.4476e-02, -1.5782e-02, -7.7333e-02, -3.7391e-03, -4.2564e-02,\n",
       "           -9.0198e-03,  6.8374e-05, -7.8573e-03,  7.9945e-03,  2.5597e-02,\n",
       "            2.5885e-03, -3.0118e-02],\n",
       "          [ 2.4358e-02, -1.5830e-02, -7.7028e-02, -4.2530e-03, -4.2757e-02,\n",
       "           -8.8596e-03,  1.3725e-04, -7.8344e-03,  7.9519e-03,  2.5734e-02,\n",
       "            2.6179e-03, -2.9731e-02],\n",
       "          [ 2.4393e-02, -1.5801e-02, -7.7709e-02, -4.0120e-03, -4.2707e-02,\n",
       "           -8.8274e-03,  9.8837e-05, -7.7971e-03,  8.0314e-03,  2.5697e-02,\n",
       "            2.5668e-03, -2.9946e-02],\n",
       "          [ 2.4576e-02, -1.5792e-02, -7.6812e-02, -3.5746e-03, -4.2375e-02,\n",
       "           -9.1060e-03,  1.3852e-04, -7.6325e-03,  7.9409e-03,  2.5374e-02,\n",
       "            2.6218e-03, -3.0367e-02],\n",
       "          [ 2.4480e-02, -1.5807e-02, -7.7054e-02, -3.9441e-03, -4.2491e-02,\n",
       "           -8.9511e-03,  1.8782e-04, -7.8159e-03,  7.9569e-03,  2.5487e-02,\n",
       "            2.6257e-03, -3.0018e-02],\n",
       "          [ 2.4498e-02, -1.5793e-02, -7.7180e-02, -3.6748e-03, -4.2502e-02,\n",
       "           -9.0102e-03,  1.5495e-04, -7.7170e-03,  7.9430e-03,  2.5445e-02,\n",
       "            2.5999e-03, -3.0267e-02],\n",
       "          [ 2.4366e-02, -1.5787e-02, -7.7652e-02, -3.8499e-03, -4.2877e-02,\n",
       "           -8.9335e-03, -2.4463e-05, -7.8062e-03,  8.0146e-03,  2.5814e-02,\n",
       "            2.5430e-03, -3.0026e-02]]], device='cuda:0', grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a56d5c7045a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Language learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAH/xJREFUeJzt3XuUnHWd5/H3t6r6ku6ku5N05x4IuchNDElaICAoMuqAHhl2cA6edRxwdhgU9TiO7nHXcxxXnYvjuHNQdsDI6sgsOjOiuCqIOCoblAQI5AISLkkIpkMu3RX6fu/67h/PU5VK20kqpJ96nur+vM6p0089z6+e+v1Snfr077n8fubuiIiIAKTiroCIiCSHQkFERAoUCiIiUqBQEBGRAoWCiIgUKBRERKRAoSAiIgUKBRERKVAoiIhIQSbuCpyq5uZmX7ZsWdzVEBGpKE8++WSHu7ecrFzFhcKyZcvYsmVL3NUQEakoZvZyKeV0+EhERAoUCiIiUqBQEBGRAoWCiIgUKBRERKRAoSAiIgUKBRERKVAoiIhUgNv+40UeebE98vdRKIiIJNxYzrnt5y/w+EtHIn8vhYKISMJ19g+Tc5hbXx35eykUREQSLts3DMDcmTWRv5dCQUQk4Tp6hwCYO1M9BRGRaS/bG/QUmtVTEBGRbL6noHMKIiKS7RsmZdBUp1AQEZn2OnqHmVNfTTplkb9XZKFgZmeb2baiR7eZfWxcGTOzr5jZLjPbYWZro6qPiEilyvYOMbc++vMJEOHMa+7+PHAhgJmlgf3AfeOKXQ2sCh8XA3eEP0VEJHSkb7gsVx5B+Q4fXQXsdvfx08FdC9ztgc1Ak5ktLFOdREQqQrZvuCz3KED5QuEG4DsTrF8M7Ct63hauExGRUEfvUFmuPIIyhIKZVQPvBr57Gvu42cy2mNmW9vboB4QSEUmKodExegZHaZ5Ch4+uBp5y90MTbNsPLC16viRcdwx33+Dure7e2tLSElE1RUSS50gZh7iA8oTCe5n40BHAD4H3h1chXQJ0ufuBMtRJRKQi5O9mLtfho8iuPgIws3rgbcCfF627BcDd7wQeAK4BdgH9wE1R1kdEpNIcHfeowi9JBXD3PmDuuHV3Fi07cGuUdRARqWRHxz2aOucURETkNcr2lbenoFAQEUmwbO8wNZkU9dXpsryfQkFEJME6eodpnlmDWfTjHoFCQUQk0bJ9Q2Ub4gIUCiIiiZbtHS7b5aigUBARSbRs71DZTjKDQkFEJLHcnY4yjpAKCgURkcTqHRpleDRHc5nmUgCFgohIYhWGuFBPQURE8jeuzdGJZhER6SgMcaHDRyIi054OH4mISMERHT4SEZG8jt5hZtVmqMmUZ9wjUCiIiCRWtm+4rOcTQKEgIpJY2d6hsg5xAQoFEZHEyvaW925mUCiIiCRWMEKqDh+JiEx7YznnSN8wzTp8JCIinf3D5Lx803DmKRRERBIo21f+G9dAoSAikkgdvcGNa3PLOEIqKBRERBIpWxj3SD0FEZFpL5vvKeicgoiIZPuGSRk0zagq6/sqFEREEqijd5g59TWkUlbW91UoiIgkULZ3qOznE0ChICKSSNm+8g9xAQoFEZFECgbDK+9JZlAoiIgkUhyD4YFCQUQkcQZHxugZGi37XAqgUBARSZwj+SEuyjwYHkQcCmbWZGb3mtlzZrbTzNaP2/4WM+sys23h4zNR1kdEpBLkQ6GcczPnZSLe/23Ag+5+vZlVA3UTlHnE3d8VcT1ERCpGZ/8IAE11UygUzKwRuAK4EcDdh4HhqN5PRGSq6BrIh0J572aGaA8fnQW0A980s61mdpeZ1U9Qbr2ZbTezn5jZ+RHWR0SkInQOBH8/N5Z5iAuINhQywFrgDndfA/QBnxpX5ingTHdfDXwV+MFEOzKzm81si5ltaW9vj7DKIiLxy/cUplootAFt7v5Y+PxegpAocPdud+8Nlx8AqsysefyO3H2Du7e6e2tLS0uEVRYRiV9X/wg1mRS1Vemyv3dkoeDuB4F9ZnZ2uOoq4NniMma2wMwsXL4orE82qjqJiFSCroGRWM4nQPRXH30EuCe88mgPcJOZ3QLg7ncC1wMfNLNRYAC4wd094jqJiCRaZ/9ILIeOIOJQcPdtQOu41XcWbb8duD3KOoiIVJqugRGaZpT/clTQHc0iIonTOTBCQ0w9BYWCiEjCdA/Ed/hIoSAikjCd/cOxnWhWKIiIJMjIWI6+4TH1FEREJN4hLkChICKSKHHezQwKBRGRRMmPkKpQEBERutVTEBGRvPwIqXHMpQAKBRGRROnS4SMREcnrGhgFoKE26qHpJqZQEBFJkM6BYWbVZMik4/l6ViiIiCRIV4zjHoFCQUQkUbr645tLARQKIiKJ0hXjYHigUBARSZTOGGddA4WCiEiiqKcgIiIAuDtd/SM0xjTrGigUREQSY3Akx/BYTj0FEREpHuJCoSAiMu3FPWw2KBRERBIj7mGzQaEgIpIY6imIiEhB3COkgkJBRCQx4p6fGRQKIiKJ0TUwQjplzKyJZ9hsUCiIiCRG58AwjTOqMLPY6qBQEBFJiK6B0VjPJ4BCQUQkMTr7hxUKIiIS6I55MDxQKIiIJEanQkFERPK6Yp5LAUoMBTNbYWY14fJbzOyjZtZUwuuazOxeM3vOzHaa2fpx283MvmJmu8xsh5mtfW3NEBGpbLmcxz6XApTeU/geMGZmK4ENwFLg2yW87jbgQXc/B1gN7By3/WpgVfi4GbijxPqIiEwpPUOjuMd7NzOUHgo5dx8FrgO+6u6fBBae6AVm1ghcAfxvAHcfdvfOccWuBe72wGagycxOuF8RkakoCUNcQOmhMGJm7wX+BPhxuO5kNT8LaAe+aWZbzewuM6sfV2YxsK/oeVu4TkRkWjk6xEV8s65B6aFwE7Ae+Gt3f8nMzgL+5SSvyQBrgTvcfQ3QB3zqtVTSzG42sy1mtqW9vf217EJEJNHyE+xURE/B3Z9194+6+3fMbDYwy92/eJKXtQFt7v5Y+PxegpAotp/g/ETeknDd+Pff4O6t7t7a0tJSSpVFRCpKEgbDg9KvPnrYzBrMbA7wFPB1M/ufJ3qNux8E9pnZ2eGqq4BnxxX7IfD+8CqkS4Audz9wak0QEal8SZhgB4JDPKVodPduM/svBCeG/8rMdpTwuo8A95hZNbAHuMnMbgFw9zuBB4BrgF1AP8FhKhGRaScJE+xA6aGQCa8K+iPg06Xu3N23Aa3jVt9ZtN2BW0vdn4jIVNU1MEJNJkVtVTrWepR6ovlzwE+B3e7+hJktB16MrloiItNLV3/8N65BiT0Fd/8u8N2i53uAP4yqUiIi000ShriA0k80LzGz+8zscPj4npktibpyIiLTRX6CnbiVevjomwRXCi0KHz8K14mIyCQIJtiJ98Y1KD0UWtz9m+4+Gj7+GdANAyIik6QrARPsQOmhkDWz95lZOny8D8hGWTERkemkos4pAB8guBz1IHAAuB64MaI6iYhMKyNjOfqGxyqnp+DuL7v7u929xd3nufsfoKuPREQmRVKGuIDTm3nt45NWCxGRaSwpQ1zA6YWCTVotRESmsXxPoaHCQ8EnrRYiItNYVzhsdlMCQuGEdzSbWQ8Tf/kbMCOSGomITDNJGQwPThIK7j6rXBUREZmuXu1LxqxrcHqHj0REZBIc6hmkOp1idoVffSQiIpPgUNcg8xpqMIv/+h2FgohIzA52D7KgoTbuagAKBRGR2B3qHmJ+o0JBRGTac3cOdqmnICIiQPfgKAMjYwoFERGBQ92DADp8JCIicLArDIVZNTHXJKBQEBGJ0cGwp7BAPQURETmU7ynonIKIiBzsHqSproraqnTcVQEUCiIisTqUoBvXQKEgIhKrg92DiTl0BAoFEZFYHewaUk9BRERgZCxHti85Q1yAQkFEJDaHe4ZwRz0FERE5euPagsZk3LgGCgURkdgc7k7WPQqgUBARiU3hbuYEhcIJ52g+XWa2F+gBxoBRd28dt/0twP8FXgpXfd/dPxdlnUREkuJgdzAN55z6+Odmzos0FEJXunvHCbY/4u7vKkM9REQSJUnTcObp8JGISEySNA1nXtSh4MBDZvakmd18nDLrzWy7mf3EzM6PuD4iIomRpGk486IOhTe5+1rgauBWM7ti3PangDPdfTXwVeAHE+3EzG42sy1mtqW9vT3aGouIlEHSpuHMizQU3H1/+PMwcB9w0bjt3e7eGy4/AFSZWfME+9ng7q3u3trS0hJllUVEyiJp03DmRRYKZlZvZrPyy8DbgWfGlVlg4RkWM7sorE82qjqJiCRF0qbhzIvy6qP5wH3hd34G+La7P2hmtwC4+53A9cAHzWwUGABucHePsE4iIolQuJs5YT2FyELB3fcAqydYf2fR8u3A7VHVQUQkqZJ44xroklQRkVjkp+Gc15CccY9AoSAiEouD3YPMTtA0nHkKBRGRGBxK2IxreQoFEZEYHOweZEHCrjwChYKISCySNg1nnkJBRKTM8tNwzlMoiIhIEqfhzFMoiIiUWRKn4cxTKIiIlNmhBE7DmadQEBEps6QOcQEKBRGRsjvUk7xpOPMUCiIiZfZK5yDzG5M1DWeeQkFEpMx+s7+Lcxc0xF2NCSkURETKqKt/hD0dfaxe2hR3VSakUBARKaMd+zsBWL1EoSAiMu3taOsC4IIljTHXZGIKBRGRMtq2r5PlzfU0zqiKuyoTUiiIiJSJu7NtX2dizyeAQkFEpGwOdg/S3jPE6oQeOgKFgohI2WzfF5xPUE9BRETY3tZJJmWcuzCZ9yiAQkFEpGy27+vk3IUNiZuXuZhCQUSkDHI55+m2LlYvTe75BFAoiIiUxZ6OPnqGRhN701qeQkFEpAy27wvvZE7wSWZQKIiIlMX2tk7qq9OsaJkZd1VOSKEgIlIG29u6uGBJI+lU8obLLqZQEBGJ2NDoGDtf6U78+QRQKIiIRO65Az0Mj+USfz4BFAoiIpHb0VYZJ5kBMnFXoFw278nylZ+/eNztpcyKZ9gJy+an1ptos1mw/nhlgtV2TNmjy1Z4z/xzivZnQMrC5XB7yiCVf25HnxeXS5mRTh1dPlomWJ8ySKWMdOF5+DNlZML1qZSRTkE6lQrLhcvhz0wqeE1V2sikUuFyikw62EcmHZQ5ZjltVKVSpBJ+7FWkVNv2ddE8s4ZFjbVxV+Wkpk0o5NwZGctNuM09/MnEX+j5bUFZP876Y5+PfwM/poyP33zM64vfw/1o+WD5aHvwo+VzYbn8vnLuR38S3Djj4etyuaPbckVlx9wLy0mQDsOiOh8i6VRhuSpcrsqkqE4b1ZnUuHXBo6YqXM6kqMmkqalKUZNfzqTC52lqq46uq60Kfs6oTlObSVNbHewjifPpSmXYtu9VVi9prIjfoUhDwcz2Aj3AGDDq7q3jthtwG3AN0A/c6O5PRVGXS1c0c+mK5ih2PeXkQ2YsF4RE/mcuFwRH/vloLgiYsVy47M7o2NFtY7kco2PB9pGcMzqWY2QsXz7YNprLMZrzcDkoM5oLAnxkLCgzEpYbCV+f3zY86gyP5RgeHWNoJEfP4CjDo7lwXe6Y5aHRHGO51552KYMZVekgKKrSheUZVWnqqtPUVWeoq05TX/O7P2fVZKivyTCzNkNDbYaG2ipm1VZRW6WgmQ7ae4bY3d7H9euWxl2VkpSjp3Clu3ccZ9vVwKrwcTFwR/hTYmRmpI3EXzp3qsZyHgbEGIMjQVgMhoGS/5nfNjgyxmDR8sDwGAMjwWMwXO4fDta39w7RP9xP/9AY/cOj9A2PlRRAVWmjcUYVjTOqmF1Xzez6ambXVTF3Zg3NM2tonlnN3Poa5jfUsKCxllm1yZyURU5s054sAJeumBtzTUoT9+Gja4G7PThestnMmsxsobsfiLleMgWlUxb8dV8d7WBk7kEPpm9ojL6hUXrzj8FRugdH6Cn62TUwQmf/MK/2jbDvSD872oY50jfMyNjvhkp9dZr5jbUsbKxlYeOMws/Fs2ewoqWeRY0zdB4mgTbtzjKrJsP5i5I7MmqxqEPBgYfMzIGvufuGcdsXA/uKnreF6xQKUrHMLDw/kWZOffUpv97d6R4Ypb13iI7eIQ51D3Koe5ADXYMc7BrkYPcgv97VwaHuQYo7JLVVKZY3z2TFvJmcs2BW8FjYwKLGWh2mitGm3R1cvHwOmXRlXOwZdSi8yd33m9k84Gdm9py7bzzVnZjZzcDNAGecccZk11EkUcyMxroqGuuqWDnv+EMijI7laO8d4rfZfvZ09LH7cC+723vZ+ttX+dH2VwrlGmozvH5xI6uXNrF6SROrlzayoEFBUQ6vdA6wN9vP+y45M+6qlCzSUHD3/eHPw2Z2H3ARUBwK+4Hisy9LwnXj97MB2ADQ2tqakGtjROKVSafCw0gzuHj5scerewZHeOFQDzsP9PDsgW6ebuvi6xv3MBp2LZbMnsHlq5p508oWLls5l6a6U+/RyMlt2p0/n1A5F7lEFgpmVg+k3L0nXH478LlxxX4IfNjM/pXgBHOXzieInL5ZtVWsO3MO686cU1g3ODLGzgPdbNvXyabdWX68/QDfeXwfZrBmaRPXXLCQqy9YyOKmGTHWfGrZtCfL7LoqzlkwK+6qlCzKnsJ84L6wi5oBvu3uD5rZLQDufifwAMHlqLsILkm9KcL6iExrtVVp1pwxmzVnzOamy85idCzH9rZONr7Qwc+ePcQX7t/JF+7fyZozmnjnBQt59+pFzGtI/s1WSeXubNqd5ZLlcyvqAgAbfzNW0rW2tvqWLVvirobIlLO3o4/7nz7AA08f4DevdJMyuGxlM/9p7WLecf4C6qrjvlixsvw2288VX/oln7/2fP54/bK4q4OZPTn+XrGJ6FMWEQCWNddz65UrufXKlew63MsPtu7nvq37+Yt/205d9TO84/wFXLdmMZetbJ5y97BE4dHdwe1Z6yvk/oQ8hYKI/I6V82byiXeczcff9jqe2HuE+7bu5/6nD3Df1v3Mm1XDu1cv4vdfv4DVS5uoqpBLLctt054sLbNqEj+pzngKBRE5rlTKuHj5XC5ePpfPvvt8fvncYb6/dT/f2rSXu371ErNqMqxfMZfLVzWzfsVcljfPrKjj51Fxdx7dnWX98rkVd+mvQkFESlJblebq8Aqlrv4RHt3dwcYXO3jkxXYeevYQALNqM1y4tIk1S5tYt2wOF581h9qqaO8gT6Ld7X209wxVzNAWxRQKInLKGuuqCgHh7uzN9vPE3iNs29fJ1t92cvsvd5FzqKtO86aVzfzeufO58px5tMyqibvqZbGpQs8ngEJBRE6TmXFWcz1nNdfzR63Bvah9Q6M8vvcIP995iF/sPFzoSVywuJG3nN3Cm1/XwoVLmypm6IdT9ejuLIubZnDGnLq4q3LKFAoiMunqazJcefY8rjx7Hn6ts/NAD7947hAPP9/O//rlLr76i1001GZ406pmLl/VwuWrmlkyu/K+QCeSyzmb92R56znzK+58AigURCRiZsZ5ixo4b1EDH37rKrr6R/jVrg4efv4wj7zYwQNPHwRgeXM9l4chsX7FXOprKu/ryd35wv07ebV/hKvOnRd3dV6TyvtXF5GK1lhXxTvfsJB3viE4H7G7vZeNL3Sw8cV2/n1LG9/a9DJVaWPtGbPDq5qaecOSxoq49PWfHt7NN379EjddtoyrX78g7uq8JrqjWUQSY2h0jCf3vsrGFzvY+EI7zx7oBoK5JN541hzWh5fHvn5RQ+LOR9zz2Mt8+r5nuG7NYr78ntWJuzS31DuaFQoiklhH+oZ5bE+WR3dn2bQny67DvUAQEmvPnM0ly+fy5te1cP6ihliP39+/4wAf/s5TXHn2PL72x+sS2atRKIjIlHO4Z5DHXzrCY3uO8PhLR3j+UA8A8xtqeOs583jrOfNZv2IuM8t4PuJXL3Zw0z8/zoVLm7j7AxdHPrPfa6VQEJEpr6N3iIefb+cXzx1i4wsd9A6NkjI4b1EDrWfO4aKz5rDuzNnMj2i016fburhhwyaWzqnj3/58PY0zkjuPtkJBRKaV4dEcW/YeYfNLR3jipSNs3fcqgyM5IOhJvGFJE6uXNHLh0tmsPbPptEd93dvRx/V3PkpNJs33P3RpZMEzWTRKqohMK9WZFJeubObSlcEsZ8OjOZ55pYttv+1kR1snO9q6+Fl4E10mZbxhSSOXhCeu1505+5QOOR3uGeT933icsZxz959elPhAOBXqKYjItNE1MMK2fZ08tifL5j1ZdrR1MZrzYw45vXHZHM5qrmd+Qw2z66oLVxG5OwMjY7T3DPGhe55iT3sf3/6zi1lzxuyYW1UaHT4SETmJ/uFRnnz5VZ7Y++rvHHKCoEfRMqsGA7J9wwyNBtvSKeOuP2nlyrMr5wY1HT4SETmJuupMOMxGCxAcctp5oJv9nQMc7h7kUM8Qh7uHAJg7s5rZddXMra/m/MUNnL+oMc6qR0ahICISqs6kWL20idVLm+KuSmySd4eFiIjERqEgIiIFCgURESlQKIiISIFCQUREChQKIiJSoFAQEZEChYKIiBRU3DAXZtYOvDxudSPQdZJ1J3o+0XIz0HEaVZ2oTqdSZrLaVLyu0tp0vG1JaVOp60/2uzZ++XTaVEp7TlSulM9o/Lok/F86UTl9PwRWufvJb8N294p/ABtOtu5EzydaBrZMdp1OpcxktWncuopq0/G2JaVNpa4/2e/aZLaplPacaptOti4J/5dOt03T7fvhRI+pcvjoRyWsO9Hz4y2fjlL2c6Iyk9WmyWpPqfuazDYdb1tS2lTq+lJ+18r5e3eicqV8RuPXTYU2Tbfvh+OquMNH5WJmW7yEEQUridpUGaZam6Zae2BqtilvqvQUorAh7gpEQG2qDFOtTVOtPTA12wSopyAiIkXUUxARkYJpEQpm9g0zO2xmz7yG164zs6fNbJeZfcXMrGjbR8zsOTP7jZn9/eTW+qT1mvQ2mdlnzWy/mW0LH9dMfs2PW6dIPqNw+1+amZtZ8+TVuKR6RfEZfd7MdoSfz0Nmtmjya37CekXRpi+F/492mNl9ZlbWyQwiatN7wu+FnJlV1rmH07msqlIewBXAWuCZ1/Dax4FLAAN+Alwdrr8S+A+gJnw+bwq06bPAJ6bKZxRuWwr8lODeluZKbxPQUFTmo8CdU6BNbwcy4fIXgS9OgTadC5wNPAy0lrM9p/uYFj0Fd98IHCleZ2YrzOxBM3vSzB4xs3PGv87MFhL8J9zswSd9N/AH4eYPAn/n7kPhexyOthXHiqhNsYmwPf8I/Feg7CfPomiTu3cXFa2nzO2KqE0PuftoWHQzsCTaVhwrojbtdPfny1H/yTYtQuE4NgAfcfd1wCeAf5qgzGKgreh5W7gO4HXA5Wb2mJn9PzN7Y6S1Lc3ptgngw2E3/htmNju6qpbktNpjZtcC+919e9QVPQWn/RmZ2V+b2T7gPwOfibCupZqM37u8DxD8xR23yWxTRZmWczSb2UzgUuC7RYefa05xNxlgDkHX8Y3Av5vZ8vAvhrKbpDbdAXye4K/PzwNfJvhPWnan2x4zqwP+O8GhiUSYpM8Id/808Gkz+2/Ah4G/mrRKnqLJalO4r08Do8A9k1O712Yy21SJpmUoEPSQOt39wuKVZpYGngyf/pDgS7K4K7sE2B8utwHfD0PgcTPLEYyH0h5lxU/gtNvk7oeKXvd14MdRVvgkTrc9K4CzgO3hf+wlwFNmdpG7H4y47sczGb93xe4BHiDGUGCS2mRmNwLvAq6K6w+rIpP9OVWWuE9qlOsBLKPoRBLwKPCecNmA1cd53fgTSdeE628BPhcuvw7YR3jfRwW3aWFRmb8A/rWS2zOuzF7KfKI5os9oVVGZjwD3ToE2/T7wLNBS7rZE/btHBZ5ojr0CZfrAvwMcAEYI/sL/U4K/Ih8Etoe/kJ85zmtbgWeA3cDt+S9+oBr4P+G2p4C3ToE2/QvwNLCD4C+hhZXcnnFlyh4KEX1G3wvX7yAYy2bxFGjTLoI/qraFj3JfURVFm64L9zUEHAJ+Ws42nc5DdzSLiEjBdL76SERExlEoiIhIgUJBREQKFAoiIlKgUBARkQKFgkwJZtZb5ve7y8zOm6R9jYWjnj5jZj862SihZtZkZh+ajPcWGU+XpMqUYGa97j5zEveX8aODtEWquO5m9i3gBXf/6xOUXwb82N1fX476yfSinoJMWWbWYmbfM7Mnwsdl4fqLzGyTmW01s0fN7Oxw/Y1m9kMz+wXwczN7i5k9bGb3huP931M0Xv7D+XHyzaw3HKRuu5ltNrP54foV4fOnzewLJfZmNnF0QL+ZZvZzM3sq3Me1YZm/A1aEvYsvhWU/GbZxh5n9j0n8Z5RpRqEgU9ltwD+6+xuBPwTuCtc/B1zu7msIRhn9m6LXrAWud/c3h8/XAB8DzgOWA5dN8D71wGZ3Xw1sBP6s6P1vc/cLOHY0zQmFY+tcRXA3OcAgcJ27ryWYv+PLYSh9Ctjt7he6+yfN7O3AKuAi4EJgnZldcbL3E5nIdB0QT6aH3wPOKxrpsiEcAbMR+JaZrSIYEbaq6DU/c/fisfUfd/c2ADPbRjBGzq/Gvc8wRwcPfBJ4W7i8nqNzO3wb+Ifj1HNGuO/FwE7gZ+F6A/4m/ILPhdvnT/D6t4ePreHzmQQhsfE47ydyXAoFmcpSwCXuPli80sxuB37p7teFx+cfLtrcN24fQ0XLY0z8f2bEj56cO16ZExlw9wvD4b5/CtwKfIVgvoQWYJ27j5jZXqB2gtcb8Lfu/rVTfF+R36HDRzKVPUQwkigAZpYfCrmRo0Mc3xjh+28mOGwFcMPJCrt7P8EUm39pZhmCeh4OA+FK4MywaA8wq+ilPwU+EPaCMLPFZjZvktog04xCQaaKOjNrK3p8nOALtjU8+foswXDnAH8P/K2ZbSXa3vLHgI+b2Q5gJdB1she4+1aCEVDfSzBfQquZPQ28n+BcCO6eBX4dXsL6JXd/iODw1Kaw7L0cGxoiJdMlqSIRCQ8HDbi7m9kNwHvd/dqTvU4kTjqnIBKddcDt4RVDncQ0tanIqVBPQURECnROQUREChQKIiJSoFAQEZEChYKIiBQoFEREpEChICIiBf8f6EuD27jq+AcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Must remeber that BERT is bidirectional, so we must modifythe architechture so it won't cheat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2366' class='' max='37335', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      6.34% [2366/37335 11:23<2:48:18 4.2816]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.7,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/anaconda3/envs/musical_neural_net/lib/python3.7/site-packages/fastai-1.0.40.dev0-py3.7.egg/fastai/callbacks/rnn.py\u001b[0m(27)\u001b[0;36mon_loss_begin\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     25 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m        \u001b[0;34m\"Save the extra outputs for later and only returns the true output.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mlast_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> out.shape\n",
      "*** NameError: name 'out' is not defined\n",
      "ipdb> up\n",
      "> \u001b[0;32m/home/ubuntu/anaconda3/envs/musical_neural_net/lib/python3.7/site-packages/fastai-1.0.40.dev0-py3.7.egg/fastai/callback.py\u001b[0m(216)\u001b[0;36mon_loss_begin\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    214 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_output'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    215 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 216 \u001b[0;31m            \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    217 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_output'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    218 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[0;32m/home/ubuntu/anaconda3/envs/musical_neural_net/lib/python3.7/site-packages/fastai-1.0.40.dev0-py3.7.egg/fastai/basic_train.py\u001b[0m(19)\u001b[0;36mloss_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     17 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     18 \u001b[0;31m    \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 19 \u001b[0;31m    \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     21 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> out.shape\n",
      "*** AttributeError: 'tuple' object has no attribute 'shape'\n",
      "ipdb> out\n",
      "(tensor([[[ 7.4878e-03, -5.2784e-03,  3.5817e-03,  ..., -1.1903e-03,\n",
      "           1.6944e-03,  4.1723e-03],\n",
      "         [ 6.6542e-03, -2.4281e-03,  4.8706e-03,  ...,  1.1724e-03,\n",
      "           3.1983e-03,  4.3668e-03],\n",
      "         [ 6.7628e-03, -4.0325e-03,  6.7935e-03,  ...,  1.0249e-03,\n",
      "           2.8824e-03,  3.8622e-03],\n",
      "         ...,\n",
      "         [ 5.3815e-03, -3.0283e-03,  5.2786e-03,  ..., -9.7884e-04,\n",
      "           8.1874e-04,  3.7366e-03],\n",
      "         [ 7.0387e-03, -4.5017e-03,  6.0563e-03,  ...,  1.3722e-03,\n",
      "           1.9246e-03,  3.0246e-03],\n",
      "         [ 9.3610e-03, -4.2608e-03,  4.7914e-03,  ...,  2.8972e-04,\n",
      "           4.1444e-03,  5.1735e-03]],\n",
      "\n",
      "        [[ 8.9777e-03, -5.3279e-03,  6.3367e-03,  ..., -2.1927e-03,\n",
      "           3.3105e-03,  4.5680e-03],\n",
      "         [ 8.5900e-03, -5.4835e-03,  4.8252e-03,  ..., -4.4274e-04,\n",
      "           1.1964e-03,  3.8308e-03],\n",
      "         [ 6.9813e-03, -4.5399e-03,  4.7696e-03,  ...,  1.3885e-04,\n",
      "           3.4556e-03,  5.3298e-03],\n",
      "         ...,\n",
      "         [ 9.2155e-03, -5.2126e-03,  7.8025e-03,  ..., -3.0226e-05,\n",
      "           7.8049e-04,  5.5195e-03],\n",
      "         [ 7.9839e-03, -3.6115e-03,  7.1714e-03,  ...,  4.9777e-04,\n",
      "           1.5623e-03,  4.3306e-03],\n",
      "         [ 9.5741e-03, -5.3795e-03,  7.0586e-03,  ..., -1.4825e-03,\n",
      "           3.3067e-03,  5.0171e-03]],\n",
      "\n",
      "        [[ 6.7392e-03, -6.0924e-03,  4.2759e-03,  ...,  1.5046e-03,\n",
      "           2.5268e-03,  5.5606e-03],\n",
      "         [ 7.4562e-03, -5.1208e-03,  5.3750e-03,  ..., -2.8512e-04,\n",
      "           3.7222e-03,  5.1582e-03],\n",
      "         [ 9.5994e-03, -4.5887e-03,  7.2139e-03,  ...,  2.4713e-03,\n",
      "           4.9873e-03,  4.7725e-03],\n",
      "         ...,\n",
      "         [ 7.5314e-03, -2.0562e-03,  6.3239e-03,  ...,  7.4356e-04,\n",
      "           3.0439e-03,  4.7363e-03],\n",
      "         [ 6.1919e-03, -3.2972e-03,  6.9915e-03,  ...,  6.2886e-04,\n",
      "           1.9853e-03,  5.1694e-03],\n",
      "         [ 8.2969e-03, -4.8357e-03,  8.8775e-03,  ..., -4.6631e-05,\n",
      "           1.3357e-03,  4.5112e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.2895e-03, -2.8451e-03,  3.8056e-03,  ..., -1.0281e-03,\n",
      "           1.4845e-03,  4.3487e-03],\n",
      "         [ 7.1803e-03, -4.5733e-03,  4.7403e-03,  ...,  2.9301e-04,\n",
      "           2.8060e-03,  5.8611e-03],\n",
      "         [ 8.5500e-03, -4.9359e-03,  5.1992e-03,  ..., -8.7203e-04,\n",
      "           3.8461e-03,  3.0365e-03],\n",
      "         ...,\n",
      "         [ 7.5307e-03, -3.0230e-03,  5.9537e-03,  ...,  7.1063e-04,\n",
      "           3.6044e-03,  2.8090e-03],\n",
      "         [ 7.3827e-03, -2.8256e-03,  5.6583e-03,  ...,  2.5973e-03,\n",
      "           2.4161e-03,  1.9303e-03],\n",
      "         [ 6.9288e-03, -2.4346e-03,  5.7677e-03,  ...,  1.7374e-03,\n",
      "           2.3847e-03,  5.0754e-03]],\n",
      "\n",
      "        [[ 7.5757e-03, -4.1739e-03,  4.9926e-03,  ..., -1.4055e-03,\n",
      "           1.3829e-03,  5.9358e-03],\n",
      "         [ 5.5839e-03, -3.0344e-03,  4.1900e-03,  ...,  1.4342e-03,\n",
      "           3.1159e-03,  5.6757e-03],\n",
      "         [ 8.8514e-03, -4.8636e-03,  5.2122e-03,  ...,  2.4402e-04,\n",
      "           4.7520e-03,  4.0518e-03],\n",
      "         ...,\n",
      "         [ 5.9224e-03, -7.2677e-03,  5.4695e-03,  ..., -8.5657e-04,\n",
      "           3.4379e-03,  4.8821e-03],\n",
      "         [ 9.3848e-03, -3.3303e-03,  5.4354e-03,  ...,  1.3391e-04,\n",
      "           5.0443e-03,  4.6439e-03],\n",
      "         [ 6.6797e-03, -5.0657e-03,  4.6976e-03,  ...,  1.8626e-04,\n",
      "           3.1969e-03,  5.9035e-03]],\n",
      "\n",
      "        [[ 5.8841e-03, -5.0196e-03,  6.3362e-03,  ..., -4.0215e-04,\n",
      "           4.3602e-04,  3.5314e-03],\n",
      "         [ 7.9032e-03, -2.3608e-03,  5.9053e-03,  ..., -3.9186e-04,\n",
      "           4.3661e-03,  4.7182e-03],\n",
      "         [ 7.3355e-03, -4.8519e-03,  5.9414e-03,  ...,  6.8920e-04,\n",
      "           3.7747e-03,  4.2622e-03],\n",
      "         ...,\n",
      "         [ 8.8184e-03, -3.7336e-03,  6.0688e-03,  ...,  1.5856e-03,\n",
      "           5.0199e-03,  6.3751e-03],\n",
      "         [ 8.8424e-03, -3.7742e-03,  5.8046e-03,  ...,  9.3233e-04,\n",
      "           2.1582e-03,  5.9100e-03],\n",
      "         [ 9.1900e-03, -4.4206e-03,  7.0112e-03,  ..., -1.0471e-03,\n",
      "           4.2951e-03,  3.9244e-03]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0.0080, 0.0055],\n",
      "        [0.0078, 0.0052],\n",
      "        [0.0071, 0.0058],\n",
      "        [0.0068, 0.0039],\n",
      "        [0.0084, 0.0063],\n",
      "        [0.0081, 0.0059],\n",
      "        [0.0066, 0.0055],\n",
      "        [0.0066, 0.0051]], device='cuda:0', grad_fn=<AddmmBackward>))\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
