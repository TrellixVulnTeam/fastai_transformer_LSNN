{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from fastai import *        # Quick accesss to most common functionality\n",
    "from fastai.text import *   # Quick accesss to NLP functionality\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from here:  \n",
    "https://gist.github.com/bearpelican/48cd4c505aea7c94e7c1e6e5e24bfac0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikitext 103\n",
    "This notebook is for training the language model on most of Wikipedia.  \n",
    "The idea is to create a generalized language model before we fine tune it on a specialized task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - Wikitext-103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset [here](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip) and unzip it so it's in the folder wikitext.\n",
    "\n",
    "Blog:\n",
    "https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset\n",
    "\n",
    "Original notebook:\n",
    "https://github.com/fastai/fastai_docs/blob/master/dev_nb/007_wikitext_2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small helper function to read the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path('data/wikitext-103-raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_url('https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip', PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fn:PathOrStr, enc='utf-8'):\n",
    "    \"Read the text in `fn`.\"\n",
    "#     with open(fn,'r', encoding = enc) as f: return ''.join(f.read().splitlines())\n",
    "    tokens = []\n",
    "    with open(fn,'r', encoding = enc) as f: \n",
    "        for line in f.read().splitlines():\n",
    "            l = line.strip()\n",
    "            if len(l) == 0: continue\n",
    "            tokens.append(l)#.split())\n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenLargeFileProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that opens the filenames and read the texts.\"\n",
    "#     def process(self, ds:Collection):        ds.items = [self.process_one(item) for item in ds.items]\n",
    "#     def process_one(self,item): return open_3(item) if isinstance(item, Path) else item\n",
    "    def process(self, ds:Collection): ds.items = read_file(ds.items[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [OpenLargeFileProcessor(), \n",
    "     TokenizeProcessor(chunksize=1000), \n",
    "     NumericalizeProcessor(vocab=None, max_vocab=5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (TextList.from_folder(PATH, recurse=True, extensions=['.raw'], processor=p)\n",
    "        .split_by_folder()\n",
    "        .label_for_lm()\n",
    "        .databunch(bs=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save('tmp_5k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikitext-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path('data/wikitext-2-raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_url('https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip', PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [OpenLargeFileProcessor(), \n",
    "     TokenizeProcessor(chunksize=1000), \n",
    "     NumericalizeProcessor(vocab=None, max_vocab=200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (TextList.from_folder(PATH, recurse=True, extensions=['.raw'], processor=p)\n",
    "        .split_by_folder()\n",
    "        .label_for_lm()\n",
    "        .databunch(bs=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save('tmp_200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([100, 95]), torch.Size([100, 95])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob = data.one_batch(); [x.shape for x in ob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <col width='5%'>  <col width='95%'>  <tr>\n",
       "    <th>idx</th>\n",
       "    <th>text</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>xxbos xxmaj polka xxmaj party ! received mixed to negative reviews from critics . xxmaj allmusic reviewer xxmaj eugene xxmaj xxunk gave the album three stars and wrote that \" just about anyone could feel let down by this album . \" xxmaj xxunk was largely critical of the parody choices , noting that many of the original versions would be forgotten in \" fifteen years \" . xxmaj christopher</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>the influence continued through the 20th century . xxmaj gerald xxmaj kelly recalled xxmaj bertin when painting his restless and confined series of portraits of xxmaj ralph xxmaj vaughan xxmaj williams in 1952 – 61 . xxmaj in 1975 xxmaj xxunk xxmaj xxunk produced a series of nine black and white photographs on board based on xxmaj ingres ' portraits of xxmaj bertin and xxmaj xxunk xxmaj caroline xxmaj rivière</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>) , a life - saving organisation equipped with technologically - advanced land , sea , air and space rescue craft ; these are headed by a fleet of five vehicles named the xxmaj thunderbirds and launched from xxup ir 's secret base in the xxmaj pacific xxmaj ocean . xxmaj the main characters are ex - astronaut xxmaj jeff xxmaj tracy , the founder of xxup ir , and</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>xxmaj open , losing to xxmaj xxunk in the quarterfinals and xxunk his xxmaj no . 1 ranking , having been just one week away from xxunk xxmaj pete xxmaj sampras 's record of 286 weeks as world xxmaj no . 1 . xxmaj in a huge upset at xxmaj wimbledon , xxmaj federer lost in the quarterfinals to xxmaj tomáš xxmaj berdych and fell to xxmaj no . 3</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>1 , with xxmaj don xxmaj bradman scoring xxunk runs at a batting average of xxunk , an aggregate record that still stands . xxmaj by the time of the next xxmaj ashes series of 1932 – 33 , xxmaj bradman 's average xxunk around 100 , approximately twice that of all other world - class batsmen . xxmaj england feared that without xxunk to drastic tactics , they might</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextLMDataBunch.load(PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
