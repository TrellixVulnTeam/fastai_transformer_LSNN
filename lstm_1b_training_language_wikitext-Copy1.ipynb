{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from fastai import *        # Quick accesss to most common functionality\n",
    "from fastai.text import *   # Quick accesss to NLP functionality\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikitext 103 (Optional)\n",
    "This notebook is for training the language model on most of Wikipedia.  \n",
    "The idea is to create a generalized language model before we fine tune it to predict snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path('data/wikitext-103')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset [here](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip) and unzip it so it's in the folder wikitext.\n",
    "\n",
    "Blog:\n",
    "https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset\n",
    "\n",
    "Original notebook:\n",
    "https://github.com/fastai/fastai_docs/blob/master/dev_nb/007_wikitext_2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small helper function to read the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url('https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip', PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    tokens = []\n",
    "    with open(PATH/filename, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            tokens.append(line.split() + [EOS])\n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = read_file('wiki.train.tokens')\n",
    "valid_tok = read_file('wiki.valid.tokens')\n",
    "test_tok = read_file('wiki.test.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1801350, 3760, 4358)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tok), len(valid_tok), len(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_tok[4][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(str(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = Vocab.create(PATH, train_tok, max_vocab=60000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array([vocab.numericalize(p) for p in train_tok])\n",
    "valid_ids = np.array([vocab.numericalize(p) for p in valid_tok])\n",
    "np.save(PATH / 'train_ids.npy', train_ids)\n",
    "np.save(PATH / 'valid_ids.npy', valid_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = TextLMDataBunch.from_id_files(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner.language_model(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt_fn = partial(optim.Adam, betas=(0.8,0.99))\n",
    "learn.callback_fns.extend([partial(GradientClipping, clip=0.12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 12\n",
    "lr = 1e-3\n",
    "momentum = (0.8,0.7)\n",
    "weight_decay = 1.2e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=4), HTML(value='0.00% [0/4 00:00<00:00]'))), HTML(valueâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 3:23:54\n",
      "epoch  train loss  valid loss  accuracy\n",
      "0      3.987870    3.370073    0.382919  (50:58)\n",
      "1      3.966082    3.356367    0.385371  (50:57)\n",
      "2      3.893526    3.352040    0.385390  (51:02)\n",
      "3      3.897372    3.351799    0.385498  (50:56)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(epoch, lr, moms=momentum, wd=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('lstm_wt103_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('lstm_wt103_full_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing (for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('lstm_wt103_full_v2'); learn.model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_vocab = text_data.train_ds.vocab\n",
    "tokenizer = text_data.train_ds.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_str = \"The online encyclopedia project Wikipedia is the most popular wiki-based website, and is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.process_all([x_str])\n",
    "x_num = lm_vocab.numericalize(tokens[0])\n",
    "x_t = torch.tensor(x_num).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the online encyclopedia project wikipedia is the most popular xxunk - based website , and is'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_vocab.textify(x_num) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem: The online encyclopedia project Wikipedia is the most popular wiki-based website, and is\n",
      "\n",
      "Predicted: the online encyclopedia project wikipedia is the most popular xxunk - based website , and is the best selling of all the year 's best selling . \" the magazine has been the subject of several publications , including the \" the best of the world \" , the \" the best of the world \" , the \" the best of the world \" , the \" best of the world \" , the \" best of the year \" list , and the \" best of the year \" list . the magazine was also ranked the best magazine in the world by the magazine 's editor , and the magazine 's \" best of the year \" list . the magazine was ranked the best magazine in the world in the first half of the 20th century , and was ranked the best magazine in the world in the late 1980s . the magazine was ranked the best magazine in the world in the 1990s , and was ranked the best magazine in the world in the 2000s . the magazine was ranked the best magazine in the world in 2006 , and the magazine 's \" best of the year \" list in 2007 . the magazine was ranked the 13th best\n"
     ]
    }
   ],
   "source": [
    "num_preds = 50\n",
    "for i in range(num_preds):\n",
    "    res,*_ = learn.model(x_t.unsqueeze(-1).cuda())\n",
    "    p1, p2 = res[-1].topk(2)[-1].detach()\n",
    "    best = p2 if p1.data == 0 else p1 # force it to not predict unknowns\n",
    "    x_t = torch.cat((x_t, best.unsqueeze(0)))\n",
    "print('Stem:', x_str)\n",
    "preds = lm_vocab.textify(x_t[-num_preds:])\n",
    "print('\\nPredicted:', preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
